from transformers import AutoModelForCausalLM, AutoTokenizer
from vllm import LLM, SamplingParams
import json
import torch,os
import argparse
os.environ["CUDA_VISIBLE_DEVICES"] = '0,1'

def parse_args():
    args = argparse.ArgumentParser()
    # network arguments
    args.add_argument("-data", "--data", type=str, help="input data directory")

    args.add_argument("-model_path", "--model_path", type=str,help="model_path directory")

    args.add_argument("-save_path", "--save_path", type=str, help="save_path")

    args.add_argument("-temperature", "--base_model", type=float,
                      default=0.0, help="temperature")

    args.add_argument("-top_p", "--top_p", type=float,
                      default=0.5, help="top_p")

    args.add_argument("-max_tokens", "--max_tokens", type=int,
                      default=4096, help="max_tokens")

    args.add_argument("-tensor_parallel_size", "--tensor_parallel_size", type=int,
                      default=2, help="tensor_parallel_size Size")

    args = args.parse_args()
    return args

def read_json(file):
  with open(file, 'r+') as file:
    content = file.read()
  content = json.loads(content)
  return content

def save_json(data,file):
    dict_json = json.dumps(data,indent=1)
    with open(file, 'w+',newline='\n') as file:
        file.write(dict_json)

def multi_expert_collaboration(data,model_path,save_path,temperature=0, top_p=0.95, max_tokens=4096,tensor_parallel_size=2):
    result = []
    sampling_params = SamplingParams(temperature=temperature, top_p=top_p,max_tokens=max_tokens)
    checkpoint = model_path
    llm = LLM(model=checkpoint,tokenizer=checkpoint,tensor_parallel_size=tensor_parallel_size,trust_remote_code=True)
    data = read_json(data)
    for i in range(len(data)):
        s_framework = "<s>[INST]You are a professional Operations Expert. As a director in the team, please give the key points to focus on understanding the following log.  These key points are used to build an overall framework for understanding the log. The user can fully understand the log by analyzing it step by step according to these key points. \nLogs:{} \n Therefore only the key points to focus on need to be given in the output and no detailed explanation is needed. For example: 1. parameter information; 2. log description; 3. possible causes; 4. procedure step.[/INST]".format(data[i][0])
        outputs = llm.generate([s_framework], sampling_params)
        framework_out = outputs[0].outputs[0].text

        s_executor = s_framework + framework_out + "</s><s>[INST]You are a professional Operations Engineer. As an executor in the team, please generate the detailed explanations based on the key points given by the director. Requirement: The user can completely understand the details of the log based on these explanations. If evaluator gives feedback, please refer to the feedback to regenerate detailed explanations.[/INST]"
        outputs = llm.generate([s_executor], sampling_params)
        executor_out = outputs[0].outputs[0].text

        s_evalutator = s_executor + executor_out + "</s><s>[INST]You are a fair and objective O&M expert. As a evaluator in the team, please evaluate whether generated content passes the following requirements: 1. Completeness: the content generated by executor does not exist omitting key points; 2. Consistency: the content generated by executor according to key points has no deviation from the context of the logs; 3. Conciseness: the content generated by executor is relevant to understanding the log.[/INST]"
        outputs = llm.generate([s_evalutator], sampling_params)
        evalutator_out = outputs[0].outputs[0].text

        regenerate_out = ''
        if 'false' in executor_out.strip():
            s_regenerate = s_evalutator + evalutator_out + '</s><s>[INST]You are a professional Operations Engineer, please refer to the feedback to regenerate detailed explanations. [/INST]'
            outputs = llm.generate([s_regenerate], sampling_params)
            regenerate_out = outputs[0].outputs[0].text

        result.append([data[i][0],framework_out,executor_out,evalutator_out,regenerate_out])
        save_json(result,save_path)

if __name__ == '__main__':
    args = parse_args()
    multi_expert_collaboration(args)